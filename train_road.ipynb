{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from load_dataset import Dataset\n",
    "from model.discriminator import Discriminator\n",
    "from model.generator import Generator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label, fake_label = 0, 1\n",
    "normal_label, abnormal_label = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_acc = BinaryAccuracy(threshold=Config.detection_thr).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)\n",
    "dis1 = Discriminator().to(device)\n",
    "dis2 = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('road')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.9)\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_set, batch_size=Config.batch_size, shuffle=False, num_workers=1)\n",
    "test_dataloader = DataLoader(dataset=test_set, batch_size=Config.batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_G = Adam(gen.parameters(), lr=Config.lr, betas=(Config.b1, Config.b2))\n",
    "optim_D1 = Adam(dis1.parameters(), lr=Config.lr, betas=(Config.b1, Config.b2))\n",
    "optim_D2 = Adam(dis2.parameters(), lr=Config.lr, betas=(Config.b1, Config.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    dis1.eval()\n",
    "    dis2.eval()\n",
    "    gen.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        img_path = Config.save_path + '/generated_img_samples'.format(epoch)\n",
    "        if not os.path.exists(img_path):\n",
    "            os.makedirs(img_path)\n",
    "\n",
    "        random_x = torch.randn(64, 256, 1, 1).to(device)\n",
    "        test_sample = gen(random_x).detach().cpu()\n",
    "\n",
    "        save_image(test_sample[0], '{}/{}.png'.format(img_path, epoch))\n",
    "        writer.add_image('generated_img_samples', test_sample, epoch, dataformats='NCHW')\n",
    "\n",
    "        batch_acc = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = dis1(inputs).to(device)\n",
    "\n",
    "            for out in range(len(output)):\n",
    "                if output[out] < Config.detection_thr:\n",
    "                    output[out] = dis2(inputs)[out].to(device)\n",
    "\n",
    "            output = output.to(device)\n",
    "\n",
    "            batch_acc += compute_acc(output.to(torch.float32), labels.to(torch.float32))\n",
    "\n",
    "        epoch_acc = batch_acc / len(test_dataloader)\n",
    "        print(f'Test accuracy for epoch {epoch}: {epoch_acc}')\n",
    "\n",
    "        writer.add_scalar('test_acc', epoch_acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    for epoch in range(Config.epochs):\n",
    "\n",
    "        # TODO: TRAIN\n",
    "\n",
    "        dis1.train()\n",
    "        dis2.train()\n",
    "        gen.train()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "\n",
    "            optim_G.zero_grad()\n",
    "            optim_D1.zero_grad()\n",
    "            optim_D2.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)  # batch, 1, 64, 48\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO: train generator\n",
    "\n",
    "            # labels.fill_(real_label)\n",
    "            gen_target = torch.zeros(Config.batch_size, requires_grad=False).to(device)\n",
    "\n",
    "            noise = torch.randn(Config.batch_size, 256, 1, 1).to(device)\n",
    "            fake_inputs = gen(noise).to(device)\n",
    "\n",
    "            gen_loss = criterion(dis2(fake_inputs).to(torch.float32), gen_target.to(torch.float32))\n",
    "            gen_loss.backward()\n",
    "            optim_G.step()\n",
    "\n",
    "            # TODO: train first discriminator for normal/abnormal data\n",
    "\n",
    "            dis1_output = dis1(inputs).to(device)\n",
    "\n",
    "            dis_1_loss = criterion(dis1_output.to(torch.float32), labels.to(torch.float32))\n",
    "            dis_1_loss.backward()\n",
    "            optim_D1.step()\n",
    "\n",
    "            # TODO: train second discriminator for real/fake data\n",
    "\n",
    "            # noise = torch.randn(Config.batch_size, 256, 1, 1).to(device)\n",
    "            # fake_inputs = gen(noise).to(device)\n",
    "\n",
    "            dis2_real_output = dis2(inputs).to(device)\n",
    "            real_target = torch.zeros(dis2_real_output.shape[0], requires_grad=False).to(device)\n",
    "\n",
    "            dis_2_real_loss = criterion(dis2_real_output.to(torch.float32), real_target.to(torch.float32))\n",
    "            # dis_2_real_loss.backward()\n",
    "\n",
    "            dis2_fake_output = dis2(fake_inputs.detach())\n",
    "            fake_target = torch.ones(dis2_fake_output.shape[0], requires_grad=False).to(device)\n",
    "\n",
    "            dis_2_fake_loss = criterion(dis2_fake_output.to(torch.float32), fake_target.to(torch.float32))\n",
    "            # dis_2_fake_loss.backward()\n",
    "\n",
    "            dis_2_total_loss = (dis_2_real_loss + dis_2_fake_loss) / 2\n",
    "            dis_2_total_loss.backward()\n",
    "\n",
    "            optim_D2.step()\n",
    "\n",
    "            writer.add_scalar('loss/dis1_loss', dis_1_loss.data, epoch)\n",
    "\n",
    "            writer.add_scalar('loss/dis_2_real_loss', dis_2_real_loss.data, epoch)\n",
    "            writer.add_scalar('loss/dis2_fake_loss', dis_2_fake_loss, epoch)\n",
    "            writer.add_scalar('loss/dis2_total_loss', dis_2_total_loss, epoch)\n",
    "\n",
    "            writer.add_scalar('loss/gen_loss', gen_loss.data, epoch)\n",
    "\n",
    "            gen_path = Config.save_path + '/gen/epoch_{}'.format(epoch)\n",
    "            if not os.path.exists(gen_path):\n",
    "                os.makedirs(gen_path)\n",
    "            dis1_path = Config.save_path + '/dis1/epoch_{}'.format(epoch)\n",
    "            if not os.path.exists(dis1_path):\n",
    "                os.makedirs(dis1_path)\n",
    "            dis2_path = Config.save_path + '/dis2/epoch_{}'.format(epoch)\n",
    "            if not os.path.exists(dis2_path):\n",
    "                os.makedirs(dis2_path)\n",
    "\n",
    "            torch.save(gen.state_dict(), gen_path + '/state_dict.pth')\n",
    "            torch.save(dis1.state_dict(), dis1_path + '/state_dict.pth')\n",
    "            torch.save(dis2.state_dict(), dis2_path + '/state_dict.pth')\n",
    "            torch.save(gen, gen_path + '/model.pth')\n",
    "            torch.save(dis1, dis1_path + '/model.pth')\n",
    "            torch.save(dis2, dis2_path + '/model.pth')\n",
    "\n",
    "            if batch_idx % Config.log_f == 0:\n",
    "                print(\"[Train] Epoch: {}/{}, Batch: {}/{}, D1 loss: {}, D2 loss: {}, G loss: {}\".format(epoch,\n",
    "                           Config.epochs, batch_idx, len(train_dataloader), dis_1_loss, dis_2_total_loss, gen_loss))\n",
    "\n",
    "\n",
    "        # TODO: TEST\n",
    "\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 0/30, Batch: 0/10929, D1 loss: 0.694366455078125, D2 loss: 0.6918829679489136, G loss: 0.6930721402168274\n",
      "[Train] Epoch: 0/30, Batch: 3000/10929, D1 loss: 9.530570423521567e-06, D2 loss: 0.6201072931289673, G loss: 0.3713977336883545\n",
      "[Train] Epoch: 0/30, Batch: 6000/10929, D1 loss: 1.0550903652983834e-06, D2 loss: 0.6315886974334717, G loss: 0.3431209325790405\n",
      "[Train] Epoch: 0/30, Batch: 9000/10929, D1 loss: 1.7458458501096175e-07, D2 loss: 0.6070866584777832, G loss: 0.3556692898273468\n",
      "Test accuracy for epoch 0: 0.999974250793457\n",
      "[Train] Epoch: 1/30, Batch: 0/10929, D1 loss: 5.8665712288075156e-08, D2 loss: 0.5463626980781555, G loss: 0.40931928157806396\n",
      "[Train] Epoch: 1/30, Batch: 3000/10929, D1 loss: 1.0570586184144304e-08, D2 loss: 0.5491909384727478, G loss: 0.40599238872528076\n",
      "[Train] Epoch: 1/30, Batch: 6000/10929, D1 loss: 1.9963142250389865e-09, D2 loss: 0.33076557517051697, G loss: 0.7279291749000549\n",
      "[Train] Epoch: 1/30, Batch: 9000/10929, D1 loss: 4.336236925084336e-10, D2 loss: 0.5471178293228149, G loss: 0.40770313143730164\n",
      "Test accuracy for epoch 1: 0.9997170567512512\n",
      "[Train] Epoch: 2/30, Batch: 0/10929, D1 loss: 1.812101441123204e-10, D2 loss: 0.5385007262229919, G loss: 0.41727086901664734\n",
      "[Train] Epoch: 2/30, Batch: 3000/10929, D1 loss: 5.48958413448819e-11, D2 loss: 0.5466160178184509, G loss: 0.40848761796951294\n",
      "[Train] Epoch: 2/30, Batch: 6000/10929, D1 loss: 2.2783858133479384e-11, D2 loss: 0.5498706698417664, G loss: 0.4050608277320862\n",
      "[Train] Epoch: 2/30, Batch: 9000/10929, D1 loss: 1.3001676124613226e-11, D2 loss: 0.5497819781303406, G loss: 0.40511104464530945\n",
      "Test accuracy for epoch 2: 1.0\n",
      "[Train] Epoch: 3/30, Batch: 0/10929, D1 loss: 1.0131529296075747e-11, D2 loss: 0.5490432381629944, G loss: 0.40575864911079407\n",
      "[Train] Epoch: 3/30, Batch: 3000/10929, D1 loss: 6.9665618759873205e-12, D2 loss: 0.5522242784500122, G loss: 0.40279871225357056\n",
      "[Train] Epoch: 3/30, Batch: 6000/10929, D1 loss: 5.201451248881828e-12, D2 loss: 0.5531361103057861, G loss: 0.40399807691574097\n",
      "[Train] Epoch: 3/30, Batch: 9000/10929, D1 loss: 4.373045328631697e-12, D2 loss: 0.5499880313873291, G loss: 0.4048289954662323\n",
      "Test accuracy for epoch 3: 0.995730459690094\n",
      "[Train] Epoch: 4/30, Batch: 0/10929, D1 loss: 4.042442595941775e-12, D2 loss: 0.5492865443229675, G loss: 0.4067738950252533\n",
      "[Train] Epoch: 4/30, Batch: 3000/10929, D1 loss: 3.331266469272509e-12, D2 loss: 0.6198569536209106, G loss: 0.35748210549354553\n",
      "[Train] Epoch: 4/30, Batch: 6000/10929, D1 loss: 2.8079305373907015e-12, D2 loss: 0.5256192684173584, G loss: 0.43644970655441284\n",
      "[Train] Epoch: 4/30, Batch: 9000/10929, D1 loss: 2.5785389448640395e-12, D2 loss: 0.554976761341095, G loss: 0.4022296369075775\n",
      "Test accuracy for epoch 4: 0.9978137612342834\n",
      "[Train] Epoch: 5/30, Batch: 0/10929, D1 loss: 2.494454946419533e-12, D2 loss: 0.5410584211349487, G loss: 0.41578173637390137\n",
      "[Train] Epoch: 5/30, Batch: 3000/10929, D1 loss: 2.1724848482673753e-12, D2 loss: 0.4779333472251892, G loss: 0.4924008846282959\n",
      "[Train] Epoch: 5/30, Batch: 6000/10929, D1 loss: 1.9126137305869317e-12, D2 loss: 0.5804184079170227, G loss: 0.37655723094940186\n",
      "[Train] Epoch: 5/30, Batch: 9000/10929, D1 loss: 1.820261606375051e-12, D2 loss: 0.5455408096313477, G loss: 0.41192126274108887\n",
      "Test accuracy for epoch 5: 0.999382734298706\n",
      "[Train] Epoch: 6/30, Batch: 0/10929, D1 loss: 1.7963397696413308e-12, D2 loss: 0.58115154504776, G loss: 0.37665048241615295\n",
      "[Train] Epoch: 6/30, Batch: 3000/10929, D1 loss: 1.6038706820986626e-12, D2 loss: 0.5628770589828491, G loss: 0.39237701892852783\n",
      "[Train] Epoch: 6/30, Batch: 6000/10929, D1 loss: 1.4417884204240283e-12, D2 loss: 0.5940335988998413, G loss: 0.3702884018421173\n",
      "[Train] Epoch: 6/30, Batch: 9000/10929, D1 loss: 1.3991364490595348e-12, D2 loss: 0.5739563703536987, G loss: 0.38442903757095337\n",
      "Test accuracy for epoch 6: 0.95162034034729\n",
      "[Train] Epoch: 7/30, Batch: 0/10929, D1 loss: 1.3966304241580518e-12, D2 loss: 0.5702614784240723, G loss: 0.3924146890640259\n",
      "[Train] Epoch: 7/30, Batch: 3000/10929, D1 loss: 1.2654260235153703e-12, D2 loss: 0.5545772910118103, G loss: 0.40192535519599915\n",
      "[Train] Epoch: 7/30, Batch: 6000/10929, D1 loss: 1.1531304340214876e-12, D2 loss: 0.5295149087905884, G loss: 0.43410438299179077\n",
      "[Train] Epoch: 7/30, Batch: 9000/10929, D1 loss: 1.1334817633101846e-12, D2 loss: 0.5294210314750671, G loss: 0.4272443354129791\n",
      "Test accuracy for epoch 7: 0.9982253313064575\n",
      "[Train] Epoch: 8/30, Batch: 0/10929, D1 loss: 1.140681624677009e-12, D2 loss: 0.5321672558784485, G loss: 0.4233250617980957\n",
      "[Train] Epoch: 8/30, Batch: 3000/10929, D1 loss: 1.0456635557432037e-12, D2 loss: 0.5582009553909302, G loss: 0.3968794047832489\n",
      "[Train] Epoch: 8/30, Batch: 6000/10929, D1 loss: 9.639246865977835e-13, D2 loss: 0.5695354342460632, G loss: 0.386175274848938\n",
      "[Train] Epoch: 8/30, Batch: 9000/10929, D1 loss: 9.590108655116447e-13, D2 loss: 0.6220012307167053, G loss: 0.3969593048095703\n",
      "Test accuracy for epoch 8: 0.9905349612236023\n",
      "[Train] Epoch: 9/30, Batch: 0/10929, D1 loss: 9.726178196167723e-13, D2 loss: 0.5393558740615845, G loss: 0.41695624589920044\n",
      "[Train] Epoch: 9/30, Batch: 3000/10929, D1 loss: 9.011480797682658e-13, D2 loss: 0.5360201597213745, G loss: 0.4217027425765991\n",
      "[Train] Epoch: 9/30, Batch: 6000/10929, D1 loss: 8.38791492860369e-13, D2 loss: 0.5630980134010315, G loss: 0.410013884305954\n",
      "[Train] Epoch: 9/30, Batch: 9000/10929, D1 loss: 8.42363613758057e-13, D2 loss: 0.5687493085861206, G loss: 0.4030963182449341\n",
      "Test accuracy for epoch 9: 0.9924382567405701\n",
      "[Train] Epoch: 10/30, Batch: 0/10929, D1 loss: 8.594590425632653e-13, D2 loss: 0.5126872658729553, G loss: 0.4455797076225281\n",
      "[Train] Epoch: 10/30, Batch: 3000/10929, D1 loss: 8.028024859468852e-13, D2 loss: 0.537236213684082, G loss: 0.41800299286842346\n",
      "[Train] Epoch: 10/30, Batch: 6000/10929, D1 loss: 7.524664485253352e-13, D2 loss: 0.5525338649749756, G loss: 0.4048871397972107\n",
      "[Train] Epoch: 10/30, Batch: 9000/10929, D1 loss: 7.603429604680079e-13, D2 loss: 0.5574517846107483, G loss: 0.4004066288471222\n",
      "Test accuracy for epoch 10: 0.9931327104568481\n",
      "[Train] Epoch: 11/30, Batch: 0/10929, D1 loss: 7.785651463809717e-13, D2 loss: 0.5389825105667114, G loss: 0.417381227016449\n",
      "[Train] Epoch: 11/30, Batch: 3000/10929, D1 loss: 7.305422000843109e-13, D2 loss: 0.561250627040863, G loss: 0.3950759470462799\n",
      "[Train] Epoch: 11/30, Batch: 6000/10929, D1 loss: 6.875609023099249e-13, D2 loss: 0.5898852944374084, G loss: 0.40828263759613037\n",
      "[Train] Epoch: 11/30, Batch: 9000/10929, D1 loss: 6.976397541255674e-13, D2 loss: 0.5943028926849365, G loss: 0.3672245740890503\n",
      "Test accuracy for epoch 11: 0.9991512298583984\n",
      "[Train] Epoch: 12/30, Batch: 0/10929, D1 loss: 7.162877062316664e-13, D2 loss: 0.5333632230758667, G loss: 0.4221036434173584\n",
      "[Train] Epoch: 12/30, Batch: 3000/10929, D1 loss: 6.745483620458625e-13, D2 loss: 0.5611220598220825, G loss: 0.3939228653907776\n",
      "[Train] Epoch: 12/30, Batch: 6000/10929, D1 loss: 6.370243959066824e-13, D2 loss: 0.594903826713562, G loss: 0.3644633889198303\n",
      "[Train] Epoch: 12/30, Batch: 9000/10929, D1 loss: 6.486254133623859e-13, D2 loss: 1.2559025287628174, G loss: 0.08684052526950836\n",
      "Test accuracy for epoch 12: 0.9763117432594299\n",
      "[Train] Epoch: 13/30, Batch: 0/10929, D1 loss: 6.674550235424848e-13, D2 loss: 0.9573981165885925, G loss: 0.15998387336730957\n",
      "[Train] Epoch: 13/30, Batch: 3000/10929, D1 loss: 6.304371087673122e-13, D2 loss: 0.624301552772522, G loss: 0.43130388855934143\n",
      "[Train] Epoch: 13/30, Batch: 6000/10929, D1 loss: 5.970184199441397e-13, D2 loss: 0.546751856803894, G loss: 0.4082198143005371\n",
      "[Train] Epoch: 13/30, Batch: 9000/10929, D1 loss: 6.096015619377892e-13, D2 loss: 0.5307007431983948, G loss: 0.4258767366409302\n",
      "Test accuracy for epoch 13: 0.994418740272522\n",
      "[Train] Epoch: 14/30, Batch: 0/10929, D1 loss: 6.283653068359096e-13, D2 loss: 0.4942721724510193, G loss: 0.46662449836730957\n",
      "[Train] Epoch: 14/30, Batch: 3000/10929, D1 loss: 5.94753304765383e-13, D2 loss: 0.5771729946136475, G loss: 0.3802222013473511\n",
      "[Train] Epoch: 14/30, Batch: 6000/10929, D1 loss: 5.642235810510154e-13, D2 loss: 0.5178622007369995, G loss: 0.43947213888168335\n",
      "[Train] Epoch: 14/30, Batch: 9000/10929, D1 loss: 5.77181423315476e-13, D2 loss: 0.5510268807411194, G loss: 0.4059639871120453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for epoch 14: 0.9396862387657166\n",
      "[Train] Epoch: 15/30, Batch: 0/10929, D1 loss: 5.956940669904487e-13, D2 loss: 0.6710757613182068, G loss: 0.33585408329963684\n",
      "[Train] Epoch: 15/30, Batch: 3000/10929, D1 loss: 5.648056891974229e-13, D2 loss: 1.1788508892059326, G loss: 0.09967878460884094\n",
      "[Train] Epoch: 15/30, Batch: 6000/10929, D1 loss: 5.36701379953014e-13, D2 loss: 0.5625198483467102, G loss: 0.39420291781425476\n",
      "[Train] Epoch: 15/30, Batch: 9000/10929, D1 loss: 5.499939154281208e-13, D2 loss: 0.5421832799911499, G loss: 0.4128880500793457\n",
      "Test accuracy for epoch 15: 0.9712191224098206\n",
      "[Train] Epoch: 16/30, Batch: 0/10929, D1 loss: 5.682806655804562e-13, D2 loss: 0.5792561173439026, G loss: 0.38265708088874817\n",
      "[Train] Epoch: 16/30, Batch: 3000/10929, D1 loss: 5.3955917424936e-13, D2 loss: 0.5092418789863586, G loss: 0.4491315484046936\n",
      "[Train] Epoch: 16/30, Batch: 6000/10929, D1 loss: 5.133520561764748e-13, D2 loss: 0.5598098039627075, G loss: 0.3954022228717804\n",
      "[Train] Epoch: 16/30, Batch: 9000/10929, D1 loss: 5.267738827606505e-13, D2 loss: 0.5790749788284302, G loss: 0.39394646883010864\n",
      "Test accuracy for epoch 16: 0.9545987844467163\n",
      "[Train] Epoch: 17/30, Batch: 0/10929, D1 loss: 5.447974970457237e-13, D2 loss: 0.5430175065994263, G loss: 0.4169405996799469\n",
      "[Train] Epoch: 17/30, Batch: 3000/10929, D1 loss: 5.178716613526979e-13, D2 loss: 0.5725391507148743, G loss: 0.3835534155368805\n",
      "[Train] Epoch: 17/30, Batch: 6000/10929, D1 loss: 4.932591336249958e-13, D2 loss: 0.5670145153999329, G loss: 0.393762469291687\n",
      "[Train] Epoch: 17/30, Batch: 9000/10929, D1 loss: 5.067343571661664e-13, D2 loss: 0.5504840612411499, G loss: 0.40512749552726746\n",
      "Test accuracy for epoch 17: 0.9966821074485779\n",
      "[Train] Epoch: 18/30, Batch: 0/10929, D1 loss: 5.244503832949055e-13, D2 loss: 0.5516630411148071, G loss: 0.40327945351600647\n",
      "[Train] Epoch: 18/30, Batch: 3000/10929, D1 loss: 4.989285894152484e-13, D2 loss: 0.5212163925170898, G loss: 0.4356633126735687\n",
      "[Train] Epoch: 18/30, Batch: 6000/10929, D1 loss: 4.755517811136367e-13, D2 loss: 0.556873083114624, G loss: 0.4023154079914093\n",
      "[Train] Epoch: 18/30, Batch: 9000/10929, D1 loss: 4.889519778644713e-13, D2 loss: 0.5681567788124084, G loss: 0.4073956310749054\n",
      "Test accuracy for epoch 18: 0.9832561612129211\n",
      "[Train] Epoch: 19/30, Batch: 0/10929, D1 loss: 5.063519048498222e-13, D2 loss: 0.5769079923629761, G loss: 0.38160601258277893\n",
      "[Train] Epoch: 19/30, Batch: 3000/10929, D1 loss: 4.820710345666834e-13, D2 loss: 0.6001360416412354, G loss: 0.3833281993865967\n",
      "[Train] Epoch: 19/30, Batch: 6000/10929, D1 loss: 4.598124181756647e-13, D2 loss: 0.5434624552726746, G loss: 0.42392945289611816\n",
      "[Train] Epoch: 19/30, Batch: 9000/10929, D1 loss: 4.731652895016702e-13, D2 loss: 0.6690993905067444, G loss: 0.33029380440711975\n",
      "Test accuracy for epoch 19: 0.9648405313491821\n",
      "[Train] Epoch: 20/30, Batch: 0/10929, D1 loss: 4.90295141725855e-13, D2 loss: 0.6147076487541199, G loss: 0.34736984968185425\n",
      "[Train] Epoch: 20/30, Batch: 3000/10929, D1 loss: 4.670856800295664e-13, D2 loss: 0.6181805729866028, G loss: 0.351753294467926\n",
      "[Train] Epoch: 20/30, Batch: 6000/10929, D1 loss: 4.457756321192552e-13, D2 loss: 0.5718298554420471, G loss: 0.385453462600708\n",
      "[Train] Epoch: 20/30, Batch: 9000/10929, D1 loss: 4.590233358345297e-13, D2 loss: 0.5294382572174072, G loss: 0.4346961975097656\n",
      "Test accuracy for epoch 20: 0.8928703665733337\n",
      "[Train] Epoch: 21/30, Batch: 0/10929, D1 loss: 4.758602366317088e-13, D2 loss: 0.5789655447006226, G loss: 0.38319963216781616\n",
      "[Train] Epoch: 21/30, Batch: 3000/10929, D1 loss: 4.5355917372563725e-13, D2 loss: 0.6537125110626221, G loss: 0.4023486375808716\n",
      "[Train] Epoch: 21/30, Batch: 6000/10929, D1 loss: 4.330719268440253e-13, D2 loss: 0.7734546065330505, G loss: 0.24720004200935364\n",
      "[Train] Epoch: 21/30, Batch: 9000/10929, D1 loss: 4.4620223856907393e-13, D2 loss: 0.32409006357192993, G loss: 0.7448160648345947\n",
      "Test accuracy for epoch 21: 0.9449794292449951\n",
      "[Train] Epoch: 22/30, Batch: 0/10929, D1 loss: 4.627829694979491e-13, D2 loss: 0.8259204626083374, G loss: 0.22830700874328613\n",
      "[Train] Epoch: 22/30, Batch: 3000/10929, D1 loss: 4.413157935877904e-13, D2 loss: 0.5653504729270935, G loss: 0.4014136791229248\n",
      "[Train] Epoch: 22/30, Batch: 6000/10929, D1 loss: 4.215947252764196e-13, D2 loss: 0.5285656452178955, G loss: 0.43094077706336975\n",
      "[Train] Epoch: 22/30, Batch: 9000/10929, D1 loss: 4.346291122142576e-13, D2 loss: 0.5036856532096863, G loss: 0.4627886414527893\n",
      "Test accuracy for epoch 22: 0.9867026805877686\n",
      "[Train] Epoch: 23/30, Batch: 0/10929, D1 loss: 4.5097657704572247e-13, D2 loss: 0.557359516620636, G loss: 0.3986532688140869\n",
      "[Train] Epoch: 23/30, Batch: 3000/10929, D1 loss: 4.302479325504066e-13, D2 loss: 0.5489780306816101, G loss: 0.4064604341983795\n",
      "[Train] Epoch: 23/30, Batch: 6000/10929, D1 loss: 4.111956001390249e-13, D2 loss: 0.6274384260177612, G loss: 0.4100647568702698\n",
      "[Train] Epoch: 23/30, Batch: 9000/10929, D1 loss: 4.2411137535919297e-13, D2 loss: 0.6998335719108582, G loss: 0.36304807662963867\n",
      "Test accuracy for epoch 23: 0.9824588298797607\n",
      "[Train] Epoch: 24/30, Batch: 0/10929, D1 loss: 4.4022394778998886e-13, D2 loss: 0.5509746074676514, G loss: 0.4052274227142334\n",
      "[Train] Epoch: 24/30, Batch: 3000/10929, D1 loss: 4.2012316477275935e-13, D2 loss: 0.5547466278076172, G loss: 0.40900832414627075\n",
      "[Train] Epoch: 24/30, Batch: 6000/10929, D1 loss: 4.016422340113318e-13, D2 loss: 0.5910090804100037, G loss: 0.37760496139526367\n",
      "[Train] Epoch: 24/30, Batch: 9000/10929, D1 loss: 4.1440315805621464e-13, D2 loss: 0.567874550819397, G loss: 0.3882816731929779\n",
      "Test accuracy for epoch 24: 0.9578137993812561\n",
      "[Train] Epoch: 25/30, Batch: 0/10929, D1 loss: 4.3027663680292316e-13, D2 loss: 0.6580742597579956, G loss: 0.3183680772781372\n",
      "[Train] Epoch: 25/30, Batch: 3000/10929, D1 loss: 4.107268182246965e-13, D2 loss: 0.603961706161499, G loss: 0.4259289503097534\n",
      "[Train] Epoch: 25/30, Batch: 6000/10929, D1 loss: 3.9276091060025387e-13, D2 loss: 1.6938145160675049, G loss: 0.03734728321433067\n",
      "[Train] Epoch: 25/30, Batch: 9000/10929, D1 loss: 4.0536253824094426e-13, D2 loss: 0.5595213174819946, G loss: 0.39774689078330994\n",
      "Test accuracy for epoch 25: 0.989814817905426\n",
      "[Train] Epoch: 26/30, Batch: 0/10929, D1 loss: 4.210125087097949e-13, D2 loss: 0.5610396265983582, G loss: 0.3945910334587097\n",
      "[Train] Epoch: 26/30, Batch: 3000/10929, D1 loss: 4.019937052505973e-13, D2 loss: 0.5435656309127808, G loss: 0.4120050072669983\n",
      "[Train] Epoch: 26/30, Batch: 6000/10929, D1 loss: 3.845302440179388e-13, D2 loss: 0.4885156750679016, G loss: 0.47617393732070923\n",
      "[Train] Epoch: 26/30, Batch: 9000/10929, D1 loss: 3.970213914572529e-13, D2 loss: 0.5332216620445251, G loss: 0.4222504198551178\n",
      "Test accuracy for epoch 26: 0.9997428059577942\n",
      "[Train] Epoch: 27/30, Batch: 0/10929, D1 loss: 4.1249165541601407e-13, D2 loss: 0.5708579421043396, G loss: 0.3846614956855774\n",
      "[Train] Epoch: 27/30, Batch: 3000/10929, D1 loss: 3.93989230346517e-13, D2 loss: 0.8869452476501465, G loss: 0.32419249415397644\n",
      "[Train] Epoch: 27/30, Batch: 6000/10929, D1 loss: 3.770021404433943e-13, D2 loss: 0.7797210812568665, G loss: 0.23704838752746582\n",
      "[Train] Epoch: 27/30, Batch: 9000/10929, D1 loss: 3.893949778507183e-13, D2 loss: 0.7574600577354431, G loss: 0.25001630187034607\n",
      "Test accuracy for epoch 27: 0.8406378626823425\n",
      "[Train] Epoch: 28/30, Batch: 0/10929, D1 loss: 4.0469098341530674e-13, D2 loss: 0.5522953867912292, G loss: 0.4333769679069519\n",
      "[Train] Epoch: 28/30, Batch: 3000/10929, D1 loss: 3.8664739270520554e-13, D2 loss: 0.5651086568832397, G loss: 0.4056002199649811\n",
      "[Train] Epoch: 28/30, Batch: 6000/10929, D1 loss: 3.7008078350962703e-13, D2 loss: 0.7357287406921387, G loss: 0.2628445327281952\n",
      "[Train] Epoch: 28/30, Batch: 9000/10929, D1 loss: 3.8236490254410505e-13, D2 loss: 0.5783156156539917, G loss: 0.42326271533966064\n",
      "Test accuracy for epoch 28: 0.9694958925247192\n",
      "[Train] Epoch: 29/30, Batch: 0/10929, D1 loss: 3.974945914954342e-13, D2 loss: 0.8594273328781128, G loss: 0.20758184790611267\n",
      "[Train] Epoch: 29/30, Batch: 3000/10929, D1 loss: 3.7987695671384825e-13, D2 loss: 0.5628839135169983, G loss: 0.3997951149940491\n",
      "[Train] Epoch: 29/30, Batch: 6000/10929, D1 loss: 3.637061897314442e-13, D2 loss: 0.48266300559043884, G loss: 0.48554202914237976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 29/30, Batch: 9000/10929, D1 loss: 3.7590351838201475e-13, D2 loss: 0.5465521216392517, G loss: 0.41044384241104126\n",
      "Test accuracy for epoch 29: 0.9357202053070068\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
