{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from load_dataset import Dataset\n",
    "from model.discriminator import Discriminator\n",
    "from model.generator import Generator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label, fake_label = 0, 1\n",
    "normal_label, abnormal_label = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_acc = BinaryAccuracy(threshold=Config.detection_thr).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)\n",
    "dis1 = Discriminator().to(device)\n",
    "dis2 = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.9)\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_set, batch_size=Config.batch_size, shuffle=False, num_workers=1)\n",
    "test_dataloader = DataLoader(dataset=test_set, batch_size=Config.batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_G = Adam(gen.parameters(), lr=Config.lr, betas=(Config.b1, Config.b2))\n",
    "optim_D1 = Adam(dis1.parameters(), lr=Config.lr, betas=(Config.b1, Config.b2))\n",
    "optim_D2 = Adam(dis2.parameters(), lr=Config.lr, betas=(Config.b1, Config.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    dis1.eval()\n",
    "    dis2.eval()\n",
    "    gen.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        img_path = Config.save_path + '/generated_img_samples'.format(epoch)\n",
    "        if not os.path.exists(img_path):\n",
    "            os.makedirs(img_path)\n",
    "\n",
    "        random_x = torch.randn(64, 256, 1, 1).to(device)\n",
    "        test_sample = gen(random_x).detach().cpu()\n",
    "\n",
    "        save_image(test_sample[0], '{}/{}.png'.format(img_path, epoch))\n",
    "        writer.add_image('generated_img_samples', test_sample, epoch, dataformats='NCHW')\n",
    "\n",
    "        batch_acc = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = dis1(inputs).to(device)\n",
    "\n",
    "            for out in range(len(output)):\n",
    "                if output[out] < Config.detection_thr:\n",
    "                    output[out] = dis2(inputs)[out].to(device)\n",
    "\n",
    "            output = output.to(device)\n",
    "\n",
    "            batch_acc += compute_acc(output.to(torch.float32), labels.to(torch.float32))\n",
    "\n",
    "        epoch_acc = batch_acc / len(test_dataloader)\n",
    "        print(f'Test accuracy for epoch {epoch}: {epoch_acc}')\n",
    "\n",
    "        writer.add_scalar('test_acc', epoch_acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    for epoch in range(Config.epochs):\n",
    "\n",
    "        # TODO: TRAIN\n",
    "\n",
    "        dis1.train()\n",
    "        dis2.train()\n",
    "        gen.train()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "\n",
    "            optim_G.zero_grad()\n",
    "            optim_D1.zero_grad()\n",
    "            optim_D2.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)  # batch, 1, 64, 48\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO: train generator\n",
    "\n",
    "            # labels.fill_(real_label)\n",
    "            gen_target = torch.zeros(Config.batch_size, requires_grad=False).to(device)\n",
    "\n",
    "            noise = torch.randn(Config.batch_size, 256, 1, 1).to(device)\n",
    "            fake_inputs = gen(noise).to(device)\n",
    "\n",
    "            gen_loss = criterion(dis2(fake_inputs).to(torch.float32), gen_target.to(torch.float32))\n",
    "            gen_loss.backward()\n",
    "            optim_G.step()\n",
    "\n",
    "            # TODO: train first discriminator for normal/abnormal data\n",
    "\n",
    "            dis1_output = dis1(inputs).to(device)\n",
    "\n",
    "            dis_1_loss = criterion(dis1_output.to(torch.float32), labels.to(torch.float32))\n",
    "            dis_1_loss.backward()\n",
    "            optim_D1.step()\n",
    "\n",
    "            # TODO: train second discriminator for real/fake data\n",
    "\n",
    "            # noise = torch.randn(Config.batch_size, 256, 1, 1).to(device)\n",
    "            # fake_inputs = gen(noise).to(device)\n",
    "\n",
    "            dis2_real_output = dis2(inputs).to(device)\n",
    "            real_target = torch.zeros(dis2_real_output.shape[0], requires_grad=False).to(device)\n",
    "\n",
    "            dis_2_real_loss = criterion(dis2_real_output.to(torch.float32), real_target.to(torch.float32))\n",
    "            # dis_2_real_loss.backward()\n",
    "\n",
    "            dis2_fake_output = dis2(fake_inputs.detach())\n",
    "            fake_target = torch.ones(dis2_fake_output.shape[0], requires_grad=False).to(device)\n",
    "\n",
    "            dis_2_fake_loss = criterion(dis2_fake_output.to(torch.float32), fake_target.to(torch.float32))\n",
    "            # dis_2_fake_loss.backward()\n",
    "\n",
    "            dis_2_total_loss = (dis_2_real_loss + dis_2_fake_loss) / 2\n",
    "            dis_2_total_loss.backward()\n",
    "\n",
    "            optim_D2.step()\n",
    "\n",
    "            writer.add_scalar('loss/dis1_loss', dis_1_loss.data, epoch)\n",
    "\n",
    "            writer.add_scalar('loss/dis_2_real_loss', dis_2_real_loss.data, epoch)\n",
    "            writer.add_scalar('loss/dis2_fake_loss', dis_2_fake_loss, epoch)\n",
    "            writer.add_scalar('loss/dis2_total_loss', dis_2_total_loss, epoch)\n",
    "\n",
    "            writer.add_scalar('loss/gen_loss', gen_loss.data, epoch)\n",
    "\n",
    "            gen_path = Config.save_path + '/gen/epoch_{}'.format(epoch)\n",
    "            if not os.path.exists(gen_path):\n",
    "                os.makedirs(gen_path)\n",
    "            dis1_path = Config.save_path + '/dis1/epoch_{}'.format(epoch)\n",
    "            if not os.path.exists(dis1_path):\n",
    "                os.makedirs(dis1_path)\n",
    "            dis2_path = Config.save_path + '/dis2/epoch_{}'.format(epoch)\n",
    "            if not os.path.exists(dis2_path):\n",
    "                os.makedirs(dis2_path)\n",
    "\n",
    "            torch.save(gen.state_dict(), gen_path + '/state_dict.pth')\n",
    "            torch.save(dis1.state_dict(), dis1_path + '/state_dict.pth')\n",
    "            torch.save(dis2.state_dict(), dis2_path + '/state_dict.pth')\n",
    "            torch.save(gen, gen_path + '/model.pth')\n",
    "            torch.save(dis1, dis1_path + '/model.pth')\n",
    "            torch.save(dis2, dis2_path + '/model.pth')\n",
    "\n",
    "            if batch_idx % Config.log_f == 0:\n",
    "                print(\"[Train] Epoch: {}/{}, Batch: {}/{}, D1 loss: {}, D2 loss: {}, G loss: {}\".format(epoch,\n",
    "                           Config.epochs, batch_idx, len(train_dataloader), dis_1_loss, dis_2_total_loss, gen_loss))\n",
    "\n",
    "\n",
    "        # TODO: TEST\n",
    "\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 0/30, Batch: 0/7282, D1 loss: 0.6960949301719666, D2 loss: 0.6928837299346924, G loss: 0.6931027173995972\n",
      "[Train] Epoch: 0/30, Batch: 3000/7282, D1 loss: 0.004239245317876339, D2 loss: 0.5627424120903015, G loss: 0.39344072341918945\n",
      "[Train] Epoch: 0/30, Batch: 6000/7282, D1 loss: 0.001236495329067111, D2 loss: 0.5537540912628174, G loss: 0.4013541638851166\n",
      "Test accuracy for epoch 0: 0.9976466298103333\n",
      "[Train] Epoch: 1/30, Batch: 0/7282, D1 loss: 0.0007217179518193007, D2 loss: 0.5443126559257507, G loss: 0.4109313488006592\n",
      "[Train] Epoch: 1/30, Batch: 3000/7282, D1 loss: 0.00237908773124218, D2 loss: 0.5493583083152771, G loss: 0.4057232737541199\n",
      "[Train] Epoch: 1/30, Batch: 6000/7282, D1 loss: 0.0008084701257757843, D2 loss: 0.5616193413734436, G loss: 0.3935122787952423\n",
      "Test accuracy for epoch 1: 0.9978009462356567\n",
      "[Train] Epoch: 2/30, Batch: 0/7282, D1 loss: 0.0005566740874201059, D2 loss: 0.5488399863243103, G loss: 0.4060681462287903\n",
      "[Train] Epoch: 2/30, Batch: 3000/7282, D1 loss: 0.0019001442706212401, D2 loss: 0.5508502721786499, G loss: 0.40518850088119507\n",
      "[Train] Epoch: 2/30, Batch: 6000/7282, D1 loss: 0.0007005471270531416, D2 loss: 0.550319254398346, G loss: 0.4054467976093292\n",
      "Test accuracy for epoch 2: 0.9981482028961182\n",
      "[Train] Epoch: 3/30, Batch: 0/7282, D1 loss: 0.0005382142262533307, D2 loss: 0.5257308483123779, G loss: 0.43047767877578735\n",
      "[Train] Epoch: 3/30, Batch: 3000/7282, D1 loss: 0.001620621420443058, D2 loss: 0.5809305310249329, G loss: 0.3755382299423218\n",
      "[Train] Epoch: 3/30, Batch: 6000/7282, D1 loss: 0.0006260403897613287, D2 loss: 0.5598806738853455, G loss: 0.3951759934425354\n",
      "Test accuracy for epoch 3: 0.9982253313064575\n",
      "[Train] Epoch: 4/30, Batch: 0/7282, D1 loss: 0.0004641332197934389, D2 loss: 0.5106330513954163, G loss: 0.4466305375099182\n",
      "[Train] Epoch: 4/30, Batch: 3000/7282, D1 loss: 0.0014051806647330523, D2 loss: 0.566950261592865, G loss: 0.3883312940597534\n",
      "[Train] Epoch: 4/30, Batch: 6000/7282, D1 loss: 0.0005415200721472502, D2 loss: 0.5734290480613708, G loss: 0.38221505284309387\n",
      "Test accuracy for epoch 4: 0.9982253313064575\n",
      "[Train] Epoch: 5/30, Batch: 0/7282, D1 loss: 0.00045207171933725476, D2 loss: 0.5164554119110107, G loss: 0.4400363564491272\n",
      "[Train] Epoch: 5/30, Batch: 3000/7282, D1 loss: 0.0011610054643824697, D2 loss: 0.5837954878807068, G loss: 0.3729674816131592\n",
      "[Train] Epoch: 5/30, Batch: 6000/7282, D1 loss: 0.0004651725757867098, D2 loss: 0.5421491861343384, G loss: 0.41273021697998047\n",
      "Test accuracy for epoch 5: 0.998263955116272\n",
      "[Train] Epoch: 6/30, Batch: 0/7282, D1 loss: 0.00040723494021221995, D2 loss: 0.5520668625831604, G loss: 0.40432611107826233\n",
      "[Train] Epoch: 6/30, Batch: 3000/7282, D1 loss: 0.0009570664260536432, D2 loss: 0.5304906964302063, G loss: 0.42503830790519714\n",
      "[Train] Epoch: 6/30, Batch: 6000/7282, D1 loss: 0.0003986050433013588, D2 loss: 0.5362749695777893, G loss: 0.418850839138031\n",
      "Test accuracy for epoch 6: 0.9983025193214417\n",
      "[Train] Epoch: 7/30, Batch: 0/7282, D1 loss: 0.0003541033365763724, D2 loss: 0.5576021075248718, G loss: 0.39735138416290283\n",
      "[Train] Epoch: 7/30, Batch: 3000/7282, D1 loss: 0.0008008291479200125, D2 loss: 0.5422099828720093, G loss: 0.4126753807067871\n",
      "[Train] Epoch: 7/30, Batch: 6000/7282, D1 loss: 0.0003405733732506633, D2 loss: 0.517629861831665, G loss: 0.43872642517089844\n",
      "Test accuracy for epoch 7: 0.998263955116272\n",
      "[Train] Epoch: 8/30, Batch: 0/7282, D1 loss: 0.0003305378486402333, D2 loss: 0.579360842704773, G loss: 0.37673258781433105\n",
      "[Train] Epoch: 8/30, Batch: 3000/7282, D1 loss: 0.0007032779976725578, D2 loss: 0.7031134366989136, G loss: 0.3697470426559448\n",
      "[Train] Epoch: 8/30, Batch: 6000/7282, D1 loss: 0.00029391725547611713, D2 loss: 0.5409957766532898, G loss: 0.4233984351158142\n",
      "Test accuracy for epoch 8: 0.998263955116272\n",
      "[Train] Epoch: 9/30, Batch: 0/7282, D1 loss: 0.00031536712776869535, D2 loss: 0.5211794376373291, G loss: 0.4389636516571045\n",
      "[Train] Epoch: 9/30, Batch: 3000/7282, D1 loss: 0.0006095750140957534, D2 loss: 0.5208688378334045, G loss: 0.4372471272945404\n",
      "[Train] Epoch: 9/30, Batch: 6000/7282, D1 loss: 0.0002562000008765608, D2 loss: 0.5502487421035767, G loss: 0.40924298763275146\n",
      "Test accuracy for epoch 9: 0.998263955116272\n",
      "[Train] Epoch: 10/30, Batch: 0/7282, D1 loss: 0.0003191326395608485, D2 loss: 0.5853801965713501, G loss: 0.37587714195251465\n",
      "[Train] Epoch: 10/30, Batch: 3000/7282, D1 loss: 0.0005415034247562289, D2 loss: 0.5761126279830933, G loss: 0.3837619423866272\n",
      "[Train] Epoch: 10/30, Batch: 6000/7282, D1 loss: 0.0002296851307619363, D2 loss: 0.5831537246704102, G loss: 0.3755100965499878\n",
      "Test accuracy for epoch 10: 0.998263955116272\n",
      "[Train] Epoch: 11/30, Batch: 0/7282, D1 loss: 0.0003198654158040881, D2 loss: 0.5918267369270325, G loss: 0.36924171447753906\n",
      "[Train] Epoch: 11/30, Batch: 3000/7282, D1 loss: 0.0004921889631077647, D2 loss: 0.565058171749115, G loss: 0.39688223600387573\n",
      "[Train] Epoch: 11/30, Batch: 6000/7282, D1 loss: 0.00020799781486857682, D2 loss: 0.5453107357025146, G loss: 0.41447582840919495\n",
      "Test accuracy for epoch 11: 0.9983025193214417\n",
      "[Train] Epoch: 12/30, Batch: 0/7282, D1 loss: 0.0003227589186280966, D2 loss: 0.5883728861808777, G loss: 0.3745301067829132\n",
      "[Train] Epoch: 12/30, Batch: 3000/7282, D1 loss: 0.0004519358044490218, D2 loss: 0.5525550246238708, G loss: 0.40662896633148193\n",
      "[Train] Epoch: 12/30, Batch: 6000/7282, D1 loss: 0.00019085482927039266, D2 loss: 0.5931764841079712, G loss: 0.3703288435935974\n",
      "Test accuracy for epoch 12: 0.9983025193214417\n",
      "[Train] Epoch: 13/30, Batch: 0/7282, D1 loss: 0.00032322874176315963, D2 loss: 0.5778953433036804, G loss: 0.38689297437667847\n",
      "[Train] Epoch: 13/30, Batch: 3000/7282, D1 loss: 0.00040372053626924753, D2 loss: 0.5074752569198608, G loss: 0.4614608883857727\n",
      "[Train] Epoch: 13/30, Batch: 6000/7282, D1 loss: 0.0001787379151210189, D2 loss: 0.7651290893554688, G loss: 0.25823283195495605\n",
      "Test accuracy for epoch 13: 0.9980324506759644\n",
      "[Train] Epoch: 14/30, Batch: 0/7282, D1 loss: 0.0003310495230834931, D2 loss: 0.6585057973861694, G loss: 0.3395874500274658\n",
      "[Train] Epoch: 14/30, Batch: 3000/7282, D1 loss: 0.0003596042515709996, D2 loss: 0.5671316981315613, G loss: 0.4068411588668823\n",
      "[Train] Epoch: 14/30, Batch: 6000/7282, D1 loss: 0.00016787803906481713, D2 loss: 0.6341288685798645, G loss: 0.3362414836883545\n",
      "Test accuracy for epoch 14: 0.9981482028961182\n",
      "[Train] Epoch: 15/30, Batch: 0/7282, D1 loss: 0.0003343932912684977, D2 loss: 0.5703318119049072, G loss: 0.4076756238937378\n",
      "[Train] Epoch: 15/30, Batch: 3000/7282, D1 loss: 0.00032324326457455754, D2 loss: 0.5861891508102417, G loss: 0.37736770510673523\n",
      "[Train] Epoch: 15/30, Batch: 6000/7282, D1 loss: 0.00015797986998222768, D2 loss: 0.5586512684822083, G loss: 0.4061635732650757\n",
      "Test accuracy for epoch 15: 0.9982253313064575\n",
      "[Train] Epoch: 16/30, Batch: 0/7282, D1 loss: 0.00033579065348021686, D2 loss: 0.6142449378967285, G loss: 0.35303717851638794\n",
      "[Train] Epoch: 16/30, Batch: 3000/7282, D1 loss: 0.00028394715627655387, D2 loss: 0.5267588496208191, G loss: 0.43591809272766113\n",
      "[Train] Epoch: 16/30, Batch: 6000/7282, D1 loss: 0.00015227853145916015, D2 loss: 0.6239724159240723, G loss: 0.3426550030708313\n",
      "Test accuracy for epoch 16: 0.998263955116272\n",
      "[Train] Epoch: 17/30, Batch: 0/7282, D1 loss: 0.00033791176974773407, D2 loss: 0.5036107301712036, G loss: 0.46600788831710815\n",
      "[Train] Epoch: 17/30, Batch: 3000/7282, D1 loss: 0.0002522130962461233, D2 loss: 0.6786749958992004, G loss: 0.30314162373542786\n",
      "[Train] Epoch: 17/30, Batch: 6000/7282, D1 loss: 0.000146195525303483, D2 loss: 0.5100544095039368, G loss: 0.45699286460876465\n",
      "Test accuracy for epoch 17: 0.998263955116272\n",
      "[Train] Epoch: 18/30, Batch: 0/7282, D1 loss: 0.0003337099915370345, D2 loss: 0.615537703037262, G loss: 0.3629031777381897\n",
      "[Train] Epoch: 18/30, Batch: 3000/7282, D1 loss: 0.00021977693540975451, D2 loss: 1.129333734512329, G loss: 0.11330370604991913\n",
      "[Train] Epoch: 18/30, Batch: 6000/7282, D1 loss: 0.00014154805103316903, D2 loss: 0.44664162397384644, G loss: 0.5371297597885132\n",
      "Test accuracy for epoch 18: 0.9983025193214417\n",
      "[Train] Epoch: 19/30, Batch: 0/7282, D1 loss: 0.0003210366703569889, D2 loss: 0.49582934379577637, G loss: 0.4900292754173279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 19/30, Batch: 3000/7282, D1 loss: 0.00018940218433272094, D2 loss: 0.5949111580848694, G loss: 0.3680686354637146\n",
      "[Train] Epoch: 19/30, Batch: 6000/7282, D1 loss: 0.0001353346451651305, D2 loss: 0.471989244222641, G loss: 0.5098506212234497\n",
      "Test accuracy for epoch 19: 0.9983025193214417\n",
      "[Train] Epoch: 20/30, Batch: 0/7282, D1 loss: 0.00030966708436608315, D2 loss: 0.6830312609672546, G loss: 0.30486947298049927\n",
      "[Train] Epoch: 20/30, Batch: 3000/7282, D1 loss: 0.00016359535220544785, D2 loss: 0.568905770778656, G loss: 0.39439553022384644\n",
      "[Train] Epoch: 20/30, Batch: 6000/7282, D1 loss: 0.00013210423639975488, D2 loss: 0.593609094619751, G loss: 0.3694118857383728\n",
      "Test accuracy for epoch 20: 0.9983025193214417\n",
      "[Train] Epoch: 21/30, Batch: 0/7282, D1 loss: 0.00029348451062105596, D2 loss: 0.459960013628006, G loss: 0.5201161503791809\n",
      "[Train] Epoch: 21/30, Batch: 3000/7282, D1 loss: 0.00014247040962800384, D2 loss: 0.5550004839897156, G loss: 0.4086458683013916\n",
      "[Train] Epoch: 21/30, Batch: 6000/7282, D1 loss: 0.00012910444638691843, D2 loss: 0.6140386462211609, G loss: 0.35154587030410767\n",
      "Test accuracy for epoch 21: 0.9983025193214417\n",
      "[Train] Epoch: 22/30, Batch: 0/7282, D1 loss: 0.00027902403962798417, D2 loss: 0.6580905914306641, G loss: 0.3194284439086914\n",
      "[Train] Epoch: 22/30, Batch: 3000/7282, D1 loss: 0.0001240297278854996, D2 loss: 0.582336962223053, G loss: 0.38587242364883423\n",
      "[Train] Epoch: 22/30, Batch: 6000/7282, D1 loss: 0.00012454406532924622, D2 loss: 0.5977766513824463, G loss: 0.3664720058441162\n",
      "Test accuracy for epoch 22: 0.9984182715415955\n",
      "[Train] Epoch: 23/30, Batch: 0/7282, D1 loss: 0.0002589252544566989, D2 loss: 0.5318371057510376, G loss: 0.4330555200576782\n",
      "[Train] Epoch: 23/30, Batch: 3000/7282, D1 loss: 0.00010440697951707989, D2 loss: 0.565561830997467, G loss: 0.39386773109436035\n",
      "[Train] Epoch: 23/30, Batch: 6000/7282, D1 loss: 0.0001199662801809609, D2 loss: 0.6022538542747498, G loss: 0.36097484827041626\n",
      "Test accuracy for epoch 23: 0.9984568357467651\n",
      "[Train] Epoch: 24/30, Batch: 0/7282, D1 loss: 0.00024330195446964353, D2 loss: 0.5898447632789612, G loss: 0.37243062257766724\n",
      "[Train] Epoch: 24/30, Batch: 3000/7282, D1 loss: 8.963196887634695e-05, D2 loss: 0.6172020435333252, G loss: 0.34861069917678833\n",
      "[Train] Epoch: 24/30, Batch: 6000/7282, D1 loss: 0.00011366911348886788, D2 loss: 0.5340398550033569, G loss: 0.4261622428894043\n",
      "Test accuracy for epoch 24: 0.9984953999519348\n",
      "[Train] Epoch: 25/30, Batch: 0/7282, D1 loss: 0.00022088173136580735, D2 loss: 0.5372699499130249, G loss: 0.4229823648929596\n",
      "[Train] Epoch: 25/30, Batch: 3000/7282, D1 loss: 7.618936069775373e-05, D2 loss: 0.5840315222740173, G loss: 0.3765603303909302\n",
      "[Train] Epoch: 25/30, Batch: 6000/7282, D1 loss: 0.00010991359886247665, D2 loss: 0.5559073686599731, G loss: 0.4021913409233093\n",
      "Test accuracy for epoch 25: 0.9984568357467651\n",
      "[Train] Epoch: 26/30, Batch: 0/7282, D1 loss: 0.00020526065782178193, D2 loss: 0.5643506050109863, G loss: 0.3977034091949463\n",
      "[Train] Epoch: 26/30, Batch: 3000/7282, D1 loss: 6.427618791349232e-05, D2 loss: 0.696645200252533, G loss: 0.2895447015762329\n",
      "[Train] Epoch: 26/30, Batch: 6000/7282, D1 loss: 0.00010985860717482865, D2 loss: 0.4942912757396698, G loss: 0.4730992615222931\n",
      "Test accuracy for epoch 26: 0.9984568357467651\n",
      "[Train] Epoch: 27/30, Batch: 0/7282, D1 loss: 0.00019009015522897243, D2 loss: 0.5882454514503479, G loss: 0.3777713179588318\n",
      "[Train] Epoch: 27/30, Batch: 3000/7282, D1 loss: 5.4679283493896946e-05, D2 loss: 0.5579099059104919, G loss: 0.40929025411605835\n",
      "[Train] Epoch: 27/30, Batch: 6000/7282, D1 loss: 0.0001051776489475742, D2 loss: 0.5084385871887207, G loss: 0.4551093876361847\n",
      "Test accuracy for epoch 27: 0.9984568357467651\n",
      "[Train] Epoch: 28/30, Batch: 0/7282, D1 loss: 0.00016830563254188746, D2 loss: 0.5700082778930664, G loss: 0.3928186297416687\n",
      "[Train] Epoch: 28/30, Batch: 3000/7282, D1 loss: 4.655618977267295e-05, D2 loss: 0.8613935112953186, G loss: 0.1989707499742508\n",
      "[Train] Epoch: 28/30, Batch: 6000/7282, D1 loss: 9.931776730809361e-05, D2 loss: 0.6038581728935242, G loss: 0.3585065007209778\n",
      "Test accuracy for epoch 28: 0.9984182715415955\n",
      "[Train] Epoch: 29/30, Batch: 0/7282, D1 loss: 0.0001511051377747208, D2 loss: 0.47890597581863403, G loss: 0.5010179281234741\n",
      "[Train] Epoch: 29/30, Batch: 3000/7282, D1 loss: 3.918077709386125e-05, D2 loss: 0.5510515570640564, G loss: 0.41046255826950073\n",
      "[Train] Epoch: 29/30, Batch: 6000/7282, D1 loss: 9.457700798520818e-05, D2 loss: 0.9400330781936646, G loss: 0.16701726615428925\n",
      "Test accuracy for epoch 29: 0.9984182715415955\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
